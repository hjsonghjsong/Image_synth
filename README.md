# Image_synth
This is a project that generates Arkit blendshape values from 3D mesh output of [VOCA](https://github.com/TimoBolkart/voca)  
Remember to check every default arguments of python files and change to your own!

# Environment
Device: Google Cloud Platfrom n1-standard-4  
Python version: 3.6  
Pytorch version: 1.04  

# Data
Datas and labels are generated by [Live Link Face](https://apps.apple.com/us/app/live-link-face/id1495370836) recording that shot on 12 speaking subjects.  
The sentences read by subjects: [VOCASET](https://sfsu.app.box.com/folder/161567487030?s=fnngbuj8evoywsae3bk7jmwrqxc9bavx)

# Process data
Place data in a folder "data" under "Project" where it contains all download subject folder from [Box](https://sfsu.app.box.com/s/v1xq5ioc9blzhhxi38e14mmkyparza70).

Run scripts (run in this order):  
All the default arguments are set for whole dataset processing  
1. rename_sentance.py  
2. create_audio.py  
3. create_obj.py  
4. create_label.py  
5. align_frame.py  

# Training 
Specify all the arguments in train.py and run it.  
The trained checkpints with a training arguments detail(info.txt) will be stored under "model".  

# Testing
Simplely specify the folder name under "model" as --model_path and run it.  
The result and a copy of info.txt will be stored under "result"  

# Validation
![image](https://user-images.githubusercontent.com/28528165/185231048-1e9c8b8c-c5f8-468f-93f9-e677deaac2d5.png)  
There is a script for validation frame by frame along with all the blendshape parameters and calculated loss.  
To check the quilty of the NN output:  
1. export the LiveLink csv files, output csv files and mesh videos using [Blender](https://www.blender.org/) then put them in the same directory of the script.
2. Specify the output video and label video as input in meshviwer.py and run it  
